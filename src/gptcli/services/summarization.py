# src/gptcli/services/summarization.py
"""
ìë™ ìš”ì•½ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ì„œë¹„ìŠ¤.

Claude Code ìŠ¤íƒ€ì¼ì˜ "unlimited context through automatic summarization" êµ¬í˜„:
1. ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©ë¥ ì´ ì„ê³„ê°’(80%) ì´ˆê³¼ ì‹œ ìë™ ìš”ì•½ íŠ¸ë¦¬ê±°
2. ì˜¤ë˜ëœ ëŒ€í™”ë¥¼ ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ í•µì‹¬ ì •ë³´ ë³´ì¡´
3. ìš”ì•½ë³¸ë„ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ì¬ìš”ì•½ ê°€ëŠ¥ (ìµœëŒ€ 3ë ˆë²¨)
"""
from __future__ import annotations
import time
from dataclasses import dataclass, field, asdict
from typing import Any, Dict, List, Optional, Tuple, TYPE_CHECKING

from rich.console import Console
from rich.panel import Panel

import src.constants as constants

if TYPE_CHECKING:
    from src.gptcli.services.tokens import TokenEstimator
    from src.gptcli.services.ai_stream import AIStreamParser


@dataclass
class SummaryMetadata:
    """ìš”ì•½ ë©”íƒ€ë°ì´í„°"""
    created_at: str                      # ìš”ì•½ ìƒì„± ì‹œê°„
    summarized_message_count: int        # ìš”ì•½ëœ ë©”ì‹œì§€ ìˆ˜
    summarized_token_count: int          # ìš”ì•½ ì „ í† í° ìˆ˜
    summary_token_count: int             # ìš”ì•½ í›„ í† í° ìˆ˜
    compression_ratio: float             # ì••ì¶•ë¥  (summary/original)
    model_used: str                      # ìš”ì•½ì— ì‚¬ìš©ëœ ëª¨ë¸
    summary_level: int = 1               # ìš”ì•½ ë ˆë²¨ (ì¬ìš”ì•½ ì‹œ ì¦ê°€)


@dataclass
class SummaryMessage:
    """
    ìš”ì•½ ë©”ì‹œì§€ êµ¬ì¡°.
    ì¼ë°˜ ë©”ì‹œì§€ì™€ êµ¬ë¶„í•˜ê¸° ìœ„í•œ íŠ¹ë³„í•œ êµ¬ì¡°ì²´.
    """
    role: str = "assistant"
    content: str = ""
    is_summary: bool = True
    metadata: Optional[SummaryMetadata] = None

    def to_dict(self) -> Dict[str, Any]:
        """API ì „ì†¡ ë° ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        base = {"role": self.role, "content": self.content}
        if self.is_summary:
            base["_summary_meta"] = {
                "is_summary": True,
                "metadata": asdict(self.metadata) if self.metadata else None
            }
        return base

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> Optional['SummaryMessage']:
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ ìš”ì•½ ë©”ì‹œì§€ ë³µì›"""
        meta = data.get("_summary_meta")
        if not meta or not meta.get("is_summary"):
            return None
        metadata = None
        if meta.get("metadata"):
            metadata = SummaryMetadata(**meta["metadata"])
        return cls(
            role=data.get("role", "assistant"),
            content=data.get("content", ""),
            is_summary=True,
            metadata=metadata
        )


class SummarizationService:
    """
    ìë™ ìš”ì•½ ì„œë¹„ìŠ¤.

    ì£¼ìš” ê¸°ëŠ¥:
    1. check_and_summarize(): ì»¨í…ìŠ¤íŠ¸ ì„ê³„ê°’ í™•ì¸ ë° ìë™ ìš”ì•½
    2. summarize_messages(): ë©”ì‹œì§€ ëª©ë¡ì„ ìš”ì•½ìœ¼ë¡œ ë³€í™˜
    3. manual_summarize(): ìˆ˜ë™ ìš”ì•½ íŠ¸ë¦¬ê±° (/summarize ëª…ë ¹ìš©)
    """

    # ìš”ì•½ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    SUMMARY_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ëŒ€í™” ë‚´ìš©ì„ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ ëŒ€í™” ë‚´ìš©ì„ ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ ìš”ì•½í•´ì£¼ì„¸ìš”:

**ìš”ì•½ ì›ì¹™:**
1. í•µì‹¬ ì •ë³´ë§Œ ë³´ì¡´: êµ¬ì²´ì ì¸ ì½”ë“œ ë³€ê²½ì‚¬í•­, ê²°ì •ëœ ì‚¬í•­, ì¤‘ìš”í•œ ì»¨í…ìŠ¤íŠ¸
2. ì‹œê°„ìˆœ êµ¬ì¡° ìœ ì§€: ëŒ€í™” íë¦„ì´ íŒŒì•…ë˜ë„ë¡
3. ì½”ë“œ/íŒŒì¼ ì •ë³´ ë³´ì¡´: ì–¸ê¸‰ëœ íŒŒì¼ëª…, í•¨ìˆ˜ëª…, ë³€ìˆ˜ëª… ë“±ì€ ë°˜ë“œì‹œ ìœ ì§€
4. ë¶ˆí•„ìš”í•œ ì¸ì‚¬ë§, ë°˜ë³µ, ì„¤ëª… ì œê±°
5. ì‚¬ìš©ìì˜ ì›ë˜ ìš”ì²­ê³¼ AIì˜ ìµœì¢… ì‘ë‹µì˜ í•µì‹¬ë§Œ ì¶”ì¶œ

**ì¶œë ¥ í˜•ì‹:**
## ì£¼ìš” ë…¼ì˜ ì‚¬í•­
- (í•µì‹¬ í¬ì¸íŠ¸ 1)
- (í•µì‹¬ í¬ì¸íŠ¸ 2)
...

## ê²°ì •/ë³€ê²½ëœ ì‚¬í•­
- (êµ¬ì²´ì  ê²°ì •ì‚¬í•­ì´ë‚˜ ì½”ë“œ ë³€ê²½)

## ì¤‘ìš” ì»¨í…ìŠ¤íŠ¸
- (ì´í›„ ëŒ€í™”ì— í•„ìš”í•œ ë°°ê²½ ì •ë³´)

ìœ„ í˜•ì‹ì— ë§ì¶° ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”. ì´ ê¸¸ì´ëŠ” ì›ë³¸ì˜ 20-30% ì´í•˜ë¥¼ ëª©í‘œë¡œ í•©ë‹ˆë‹¤."""

    RESUMMARIZE_PROMPT = """ê¸°ì¡´ ìš”ì•½ê³¼ ìƒˆë¡œìš´ ëŒ€í™” ë‚´ìš©ì„ í†µí•©í•˜ì—¬ ë” ê°„ê²°í•œ ìš”ì•½ì„ ìƒì„±í•˜ì„¸ìš”.

**ì£¼ì˜ì‚¬í•­:**
- ê¸°ì¡´ ìš”ì•½ì˜ í•µì‹¬ ì •ë³´ëŠ” ë°˜ë“œì‹œ ìœ ì§€
- ì¤‘ë³µë˜ëŠ” ë‚´ìš©ì€ ì œê±°
- ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ ì‹œ
- ì´ ê¸¸ì´ëŠ” ì›ë³¸ì˜ 50% ì´í•˜ë¡œ ì••ì¶•

ìœ„ì˜ ì¶œë ¥ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”."""

    def __init__(
        self,
        console: Console,
        token_estimator: 'TokenEstimator',
        parser: 'AIStreamParser',
        config: Optional[Dict[str, Any]] = None
    ):
        """
        Args:
            console: Rich Console ì¸ìŠ¤í„´ìŠ¤
            token_estimator: í† í° ì¶”ì •ê¸°
            parser: API í˜¸ì¶œìš© íŒŒì„œ
            config: ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ (threshold, keep_recent ë“±)
        """
        self.console = console
        self.token_estimator = token_estimator
        self.parser = parser

        # ì„¤ì •ê°’ ë¡œë“œ
        config = config or {}
        self.threshold = config.get("threshold", constants.SUMMARIZATION_THRESHOLD)
        self.min_messages = config.get("min_messages", constants.MIN_MESSAGES_TO_SUMMARIZE)
        self.keep_recent = config.get("keep_recent", constants.KEEP_RECENT_MESSAGES)
        self.max_levels = config.get("max_levels", constants.MAX_SUMMARY_LEVELS)

        # ìš”ì•½ íˆìŠ¤í† ë¦¬ (ì„¸ì…˜ë³„)
        self.summary_history: List[SummaryMetadata] = []

    def calculate_context_usage(
        self,
        messages: List[Dict[str, Any]],
        model_context_limit: int,
        system_prompt_tokens: int,
        reserve_for_completion: int,
        tools_tokens: int = 0
    ) -> Tuple[int, int, float]:
        """
        í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©ëŸ‰ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

        Returns:
            (used_tokens, available_tokens, usage_ratio)
        """
        from src.gptcli.utils.common import Utils

        used = sum(
            Utils._count_message_tokens_with_estimator(m, self.token_estimator)
            for m in messages
        )

        available = model_context_limit - system_prompt_tokens - reserve_for_completion - tools_tokens
        ratio = used / available if available > 0 else 1.0

        return used, available, ratio

    def should_summarize(
        self,
        messages: List[Dict[str, Any]],
        model_context_limit: int,
        system_prompt_tokens: int,
        reserve_for_completion: int,
        tools_tokens: int = 0
    ) -> Tuple[bool, float, str]:
        """
        ìš”ì•½ì´ í•„ìš”í•œì§€ íŒë‹¨í•©ë‹ˆë‹¤.

        Returns:
            (should_summarize, current_ratio, reason)
        """
        used, available, ratio = self.calculate_context_usage(
            messages, model_context_limit, system_prompt_tokens,
            reserve_for_completion, tools_tokens
        )

        # ë©”ì‹œì§€ ìˆ˜ í™•ì¸
        if len(messages) < self.min_messages:
            return False, ratio, f"ë©”ì‹œì§€ ìˆ˜ ë¶€ì¡± ({len(messages)} < {self.min_messages})"

        # ì„ê³„ê°’ í™•ì¸
        if ratio < self.threshold:
            return False, ratio, f"ì„ê³„ê°’ ë¯¸ë‹¬ ({ratio:.1%} < {self.threshold:.0%})"

        # ìš”ì•½í•  ëŒ€ìƒ í™•ì¸ (ìµœê·¼ Nê°œ ì œì™¸)
        summarizable = len(messages) - self.keep_recent
        if summarizable < 2:
            return False, ratio, "ìš”ì•½í•  ë©”ì‹œì§€ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŒ"

        return True, ratio, f"ì„ê³„ê°’ ì´ˆê³¼ ({ratio:.1%} >= {self.threshold:.0%})"

    def _prepare_messages_for_summary(
        self,
        messages: List[Dict[str, Any]]
    ) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """
        ìš”ì•½í•  ë©”ì‹œì§€ì™€ ë³´ì¡´í•  ë©”ì‹œì§€ë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤.

        Returns:
            (to_summarize, to_keep)
        """
        if len(messages) <= self.keep_recent:
            return [], messages

        split_point = len(messages) - self.keep_recent
        to_summarize = messages[:split_point]
        to_keep = messages[split_point:]

        return to_summarize, to_keep

    def _format_messages_for_prompt(self, messages: List[Dict[str, Any]]) -> str:
        """ë©”ì‹œì§€ ëª©ë¡ì„ ìš”ì•½ í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        lines = []
        for i, msg in enumerate(messages):
            role = msg.get("role", "unknown")
            content = msg.get("content", "")

            # ë¦¬ìŠ¤íŠ¸ í˜•ì‹ content ì²˜ë¦¬ (ì²¨ë¶€íŒŒì¼ í¬í•¨)
            if isinstance(content, list):
                text_parts = []
                for part in content:
                    if part.get("type") == "text":
                        text_parts.append(part.get("text", ""))
                    elif part.get("type") == "image_url":
                        text_parts.append("[ì´ë¯¸ì§€ ì²¨ë¶€]")
                    elif part.get("type") == "file":
                        fname = part.get("file", {}).get("filename", "íŒŒì¼")
                        text_parts.append(f"[íŒŒì¼ ì²¨ë¶€: {fname}]")
                content = "\n".join(text_parts)

            # ì´ë¯¸ ìš”ì•½ëœ ë©”ì‹œì§€ í‘œì‹œ
            if msg.get("_summary_meta", {}).get("is_summary"):
                lines.append(f"[ë©”ì‹œì§€ {i+1}] {role.upper()} (ê¸°ì¡´ ìš”ì•½):\n{content}\n")
            else:
                lines.append(f"[ë©”ì‹œì§€ {i+1}] {role.upper()}:\n{content}\n")

        return "\n---\n".join(lines)

    # ì¼ë¶€ API (Gemini ë“±)ëŠ” ìš”ì²­ ë³¸ë¬¸ í¬ê¸° ì œí•œì´ ìˆìŒ
    # ì²­í¬ë‹¹ ìµœëŒ€ í† í° (ì•ˆì „ ë§ˆì§„ í¬í•¨)
    CHUNK_TOKEN_LIMIT: int = 25000

    def summarize_messages(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        is_resummarize: bool = False
    ) -> Optional[Tuple[str, int, int]]:
        """
        ë©”ì‹œì§€ ëª©ë¡ì„ ìš”ì•½í•©ë‹ˆë‹¤.

        ìš”ì²­ì´ ë„ˆë¬´ í¬ë©´ (413 ì—ëŸ¬ ë°©ì§€) ì²­í¬ ë¶„í•  ìš”ì•½ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        Args:
            messages: ìš”ì•½í•  ë©”ì‹œì§€ ëª©ë¡
            model: ìš”ì•½ì— ì‚¬ìš©í•  ëª¨ë¸
            is_resummarize: ì¬ìš”ì•½ ì—¬ë¶€ (ê¸°ì¡´ ìš”ì•½ + ìƒˆ ë©”ì‹œì§€)

        Returns:
            (summary_text, original_tokens, summary_tokens) ë˜ëŠ” None
        """
        from src.gptcli.utils.common import Utils

        if not messages:
            return None

        # ì›ë³¸ í† í° ìˆ˜ ê³„ì‚°
        original_tokens = sum(
            Utils._count_message_tokens_with_estimator(m, self.token_estimator)
            for m in messages
        )

        # ì²­í¬ ë¶„í•  í•„ìš” ì—¬ë¶€ í™•ì¸
        if original_tokens > self.CHUNK_TOKEN_LIMIT:
            self.console.print(
                f"[yellow]âš  ìš”ì•½ ëŒ€ìƒì´ í½ë‹ˆë‹¤ ({original_tokens:,}tk). ì²­í¬ ë¶„í•  ìš”ì•½ ì‹¤í–‰...[/yellow]",
                highlight=False
            )
            return self._chunked_summarize(messages, model, original_tokens)

        # ë‹¨ì¼ ìš”ì•½ (ê¸°ì¡´ ë¡œì§)
        return self._single_summarize(messages, model, is_resummarize, original_tokens)

    def _single_summarize(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        is_resummarize: bool,
        original_tokens: int
    ) -> Optional[Tuple[str, int, int]]:
        """ë‹¨ì¼ API í˜¸ì¶œë¡œ ìš”ì•½ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        # ìš”ì•½ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
        formatted_content = self._format_messages_for_prompt(messages)
        system_prompt = self.RESUMMARIZE_PROMPT if is_resummarize else self.SUMMARY_SYSTEM_PROMPT

        # ë¡œë”© íŒ¨ë„ í‘œì‹œ
        self.console.print(
            Panel.fit(
                f"[dim]ìš”ì•½ ì¤‘... ({len(messages)}ê°œ ë©”ì‹œì§€, ~{original_tokens:,} í† í°)[/dim]",
                title="[yellow]ğŸ“ ì»¨í…ìŠ¤íŠ¸ ì••ì¶•[/yellow]",
                border_style="yellow"
            ),
            highlight=False
        )

        try:
            # API í˜¸ì¶œ (ìŠ¤íŠ¸ë¦¬ë° ì—†ì´ ê°„ë‹¨í•˜ê²Œ)
            result = self.parser.stream_and_parse(
                system_prompt={"role": "system", "content": system_prompt},
                final_messages=[{
                    "role": "user",
                    "content": f"ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{formatted_content}"
                }],
                model=model,
                pretty_print=False,  # íŒ¨ë„ë¡œ ê°„ë‹¨íˆ
                tools=None
            )

            if result is None:
                self.console.print("[red]ìš”ì•½ ìƒì„± ì‹¤íŒ¨[/red]", highlight=False)
                return None

            summary_text, _, _ = result
            summary_tokens = self.token_estimator.count_text_tokens(summary_text)

            compression = summary_tokens / original_tokens if original_tokens > 0 else 0
            self.console.print(
                f"[green]âœ… ìš”ì•½ ì™„ë£Œ:[/green] {original_tokens:,} â†’ {summary_tokens:,} í† í° "
                f"([cyan]{compression:.1%}[/cyan] ì••ì¶•)",
                highlight=False
            )

            return summary_text, original_tokens, summary_tokens

        except Exception as e:
            self.console.print(f"[red]ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}[/red]", highlight=False)
            return None

    def _chunked_summarize(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        original_tokens: int
    ) -> Optional[Tuple[str, int, int]]:
        """
        ë©”ì‹œì§€ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ìˆœì°¨ì ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.

        ê° ì²­í¬ë¥¼ ê°œë³„ ìš”ì•½ â†’ ì¤‘ê°„ ìš”ì•½ë“¤ì„ ìµœì¢… í†µí•© ìš”ì•½
        """
        from src.gptcli.utils.common import Utils

        # 1. ë©”ì‹œì§€ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        chunks: List[List[Dict[str, Any]]] = []
        current_chunk: List[Dict[str, Any]] = []
        current_tokens = 0

        for msg in messages:
            msg_tokens = Utils._count_message_tokens_with_estimator(msg, self.token_estimator)

            # ì²­í¬ í¬ê¸° ì´ˆê³¼ ì‹œ ìƒˆ ì²­í¬ ì‹œì‘
            if current_tokens + msg_tokens > self.CHUNK_TOKEN_LIMIT and current_chunk:
                chunks.append(current_chunk)
                current_chunk = []
                current_tokens = 0

            current_chunk.append(msg)
            current_tokens += msg_tokens

        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunks.append(current_chunk)

        self.console.print(
            f"[cyan]ğŸ“¦ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í•  ìš”ì•½ ì‹œì‘[/cyan]",
            highlight=False
        )

        # 2. ê° ì²­í¬ ìš”ì•½
        chunk_summaries: List[str] = []
        for i, chunk in enumerate(chunks, 1):
            chunk_tokens = sum(
                Utils._count_message_tokens_with_estimator(m, self.token_estimator)
                for m in chunk
            )
            self.console.print(
                f"[dim]  ì²­í¬ {i}/{len(chunks)} ìš”ì•½ ì¤‘... ({len(chunk)}ê°œ, ~{chunk_tokens:,}tk)[/dim]",
                highlight=False
            )

            formatted = self._format_messages_for_prompt(chunk)

            try:
                result = self.parser.stream_and_parse(
                    system_prompt={"role": "system", "content": self.SUMMARY_SYSTEM_PROMPT},
                    final_messages=[{
                        "role": "user",
                        "content": f"ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{formatted}"
                    }],
                    model=model,
                    pretty_print=False,
                    tools=None
                )

                if result and result[0]:
                    chunk_summaries.append(f"[íŒŒíŠ¸ {i}]\n{result[0]}")
                else:
                    self.console.print(f"[yellow]ì²­í¬ {i} ìš”ì•½ ì‹¤íŒ¨, ê±´ë„ˆëœ€[/yellow]", highlight=False)

            except Exception as e:
                self.console.print(f"[yellow]ì²­í¬ {i} ì˜¤ë¥˜: {e}[/yellow]", highlight=False)

        if not chunk_summaries:
            self.console.print("[red]ëª¨ë“  ì²­í¬ ìš”ì•½ ì‹¤íŒ¨[/red]", highlight=False)
            return None

        # 3. ì²­í¬ ìš”ì•½ë“¤ì„ ìµœì¢… í†µí•© (ì²­í¬ê°€ 2ê°œ ì´ìƒì¼ ë•Œë§Œ)
        if len(chunk_summaries) == 1:
            final_summary = chunk_summaries[0].replace("[íŒŒíŠ¸ 1]\n", "")
        else:
            self.console.print(
                f"[dim]  ìµœì¢… í†µí•© ìš”ì•½ ì¤‘... ({len(chunk_summaries)}ê°œ íŒŒíŠ¸)[/dim]",
                highlight=False
            )

            combined = "\n\n---\n\n".join(chunk_summaries)

            try:
                result = self.parser.stream_and_parse(
                    system_prompt={"role": "system", "content": self.RESUMMARIZE_PROMPT},
                    final_messages=[{
                        "role": "user",
                        "content": f"ë‹¤ìŒ ë¶„í•  ìš”ì•½ë“¤ì„ í•˜ë‚˜ë¡œ í†µí•©í•´ì£¼ì„¸ìš”:\n\n{combined}"
                    }],
                    model=model,
                    pretty_print=False,
                    tools=None
                )

                if result and result[0]:
                    final_summary = result[0]
                else:
                    # í†µí•© ì‹¤íŒ¨ ì‹œ ì²­í¬ ìš”ì•½ë“¤ì„ ê·¸ëƒ¥ ì´ì–´ë¶™ì„
                    final_summary = combined

            except Exception as e:
                self.console.print(f"[yellow]í†µí•© ìš”ì•½ ì˜¤ë¥˜: {e}[/yellow]", highlight=False)
                final_summary = combined

        summary_tokens = self.token_estimator.count_text_tokens(final_summary)
        compression = summary_tokens / original_tokens if original_tokens > 0 else 0

        self.console.print(
            f"[green]âœ… ì²­í¬ ë¶„í•  ìš”ì•½ ì™„ë£Œ:[/green] {original_tokens:,} â†’ {summary_tokens:,} í† í° "
            f"([cyan]{compression:.1%}[/cyan] ì••ì¶•)",
            highlight=False
        )

        return final_summary, original_tokens, summary_tokens

    def check_and_summarize(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        model_context_limit: int,
        system_prompt_tokens: int,
        reserve_for_completion: int,
        tools_tokens: int = 0
    ) -> Tuple[List[Dict[str, Any]], bool]:
        """
        ì»¨í…ìŠ¤íŠ¸ ì„ê³„ê°’ì„ í™•ì¸í•˜ê³  í•„ìš”ì‹œ ìë™ ìš”ì•½ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        ì´ ë©”ì„œë“œëŠ” _handle_chat_message()ì—ì„œ trim_messages_by_tokens() ì „ì— í˜¸ì¶œë©ë‹ˆë‹¤.

        Args:
            messages: í˜„ì¬ ë©”ì‹œì§€ ëª©ë¡
            model: ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸
            model_context_limit: ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ í•œê³„
            system_prompt_tokens: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜
            reserve_for_completion: ì‘ë‹µ ì˜ˆì•½ í† í°
            tools_tokens: Tool ìŠ¤í‚¤ë§ˆ í† í° ìˆ˜

        Returns:
            (updated_messages, was_summarized)
        """
        should, ratio, reason = self.should_summarize(
            messages, model_context_limit, system_prompt_tokens,
            reserve_for_completion, tools_tokens
        )

        if not should:
            # ë””ë²„ê·¸ìš© (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
            # self.console.print(f"[dim]ìš”ì•½ ê±´ë„ˆëœ€: {reason}[/dim]", highlight=False)
            return messages, False

        self.console.print(
            f"\n[yellow]âš ï¸ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©ë¥  {ratio:.1%} - ìë™ ìš”ì•½ ì‹œì‘[/yellow]",
            highlight=False
        )

        # ìš”ì•½ ëŒ€ìƒê³¼ ë³´ì¡´ ëŒ€ìƒ ë¶„ë¦¬
        to_summarize, to_keep = self._prepare_messages_for_summary(messages)

        if not to_summarize:
            return messages, False

        # ê¸°ì¡´ ìš”ì•½ì´ ìˆëŠ”ì§€ í™•ì¸
        has_existing_summary = any(
            m.get("_summary_meta", {}).get("is_summary")
            for m in to_summarize
        )

        # ìš”ì•½ ë ˆë²¨ í™•ì¸ (ìµœëŒ€ ë ˆë²¨ ì´ˆê³¼ ì‹œ ê²½ê³ )
        current_level = self._get_current_summary_level(to_summarize)
        if current_level >= self.max_levels:
            self.console.print(
                f"[yellow]ìµœëŒ€ ìš”ì•½ ë ˆë²¨({self.max_levels}) ë„ë‹¬. ê¸°ì¡´ íŠ¸ë¦¬ë°ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.[/yellow]",
                highlight=False
            )
            return messages, False

        # ìš”ì•½ ìˆ˜í–‰
        result = self.summarize_messages(
            to_summarize,
            model,
            is_resummarize=has_existing_summary
        )

        if result is None:
            self.console.print(
                "[yellow]ìš”ì•½ ì‹¤íŒ¨, ê¸°ì¡´ íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì§„í–‰[/yellow]",
                highlight=False
            )
            return messages, False

        summary_text, original_tokens, summary_tokens = result

        # ìš”ì•½ ë©”ì‹œì§€ ìƒì„±
        metadata = SummaryMetadata(
            created_at=time.strftime("%Y-%m-%d %H:%M:%S"),
            summarized_message_count=len(to_summarize),
            summarized_token_count=original_tokens,
            summary_token_count=summary_tokens,
            compression_ratio=summary_tokens / original_tokens if original_tokens > 0 else 0,
            model_used=model,
            summary_level=current_level + 1
        )

        summary_msg = SummaryMessage(
            role="assistant",
            content=f"[ì´ì „ ëŒ€í™” ìš”ì•½]\n\n{summary_text}",
            is_summary=True,
            metadata=metadata
        )

        # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        self.summary_history.append(metadata)

        # ìƒˆ ë©”ì‹œì§€ ëª©ë¡ êµ¬ì„±: [ìš”ì•½] + [ë³´ì¡´ëœ ìµœê·¼ ë©”ì‹œì§€]
        new_messages = [summary_msg.to_dict()] + to_keep

        self.console.print(
            f"[green]âœ… ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ì™„ë£Œ:[/green] {len(messages)} â†’ {len(new_messages)} ë©”ì‹œì§€\n",
            highlight=False
        )

        return new_messages, True

    def _get_current_summary_level(self, messages: List[Dict[str, Any]]) -> int:
        """í˜„ì¬ ë©”ì‹œì§€ì—ì„œ ìµœëŒ€ ìš”ì•½ ë ˆë²¨ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        max_level = 0
        for msg in messages:
            meta = msg.get("_summary_meta", {})
            if meta.get("is_summary") and meta.get("metadata"):
                level = meta["metadata"].get("summary_level", 1)
                max_level = max(max_level, level)
        return max_level

    def get_summary_info(self, messages: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """
        í˜„ì¬ ë©”ì‹œì§€ì—ì„œ ìš”ì•½ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (/show_summary ëª…ë ¹ìš©)
        """
        for msg in messages:
            summary = SummaryMessage.from_dict(msg)
            if summary:
                return {
                    "content": summary.content,
                    "metadata": asdict(summary.metadata) if summary.metadata else None
                }
        return None

    def manual_summarize(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        force: bool = False
    ) -> Tuple[List[Dict[str, Any]], bool]:
        """
        ìˆ˜ë™ ìš”ì•½ (/summarize ëª…ë ¹ìš©)

        Args:
            messages: í˜„ì¬ ë©”ì‹œì§€ ëª©ë¡
            model: ì‚¬ìš©í•  ëª¨ë¸
            force: ì„ê³„ê°’/ë©”ì‹œì§€ ìˆ˜ ë¬´ì‹œí•˜ê³  ê°•ì œ ìš”ì•½

        Returns:
            (updated_messages, was_summarized)
        """
        if len(messages) < self.min_messages and not force:
            self.console.print(
                f"[yellow]ìš”ì•½í•  ë©”ì‹œì§€ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ "
                f"({len(messages)} < {self.min_messages})[/yellow]",
                highlight=False
            )
            self.console.print(
                "[dim]ê°•ì œ ìš”ì•½: /summarize --force[/dim]",
                highlight=False
            )
            return messages, False

        to_summarize, to_keep = self._prepare_messages_for_summary(messages)

        if not to_summarize:
            self.console.print("[yellow]ìš”ì•½í•  ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤[/yellow]", highlight=False)
            return messages, False

        # ê¸°ì¡´ ìš”ì•½ í™•ì¸
        has_existing_summary = any(
            m.get("_summary_meta", {}).get("is_summary")
            for m in to_summarize
        )

        # ìš”ì•½ ë ˆë²¨ í™•ì¸
        current_level = self._get_current_summary_level(to_summarize)
        if current_level >= self.max_levels and not force:
            self.console.print(
                f"[yellow]ìµœëŒ€ ìš”ì•½ ë ˆë²¨({self.max_levels}) ë„ë‹¬.[/yellow]",
                highlight=False
            )
            self.console.print(
                "[dim]ê°•ì œ ìš”ì•½: /summarize --force[/dim]",
                highlight=False
            )
            return messages, False

        result = self.summarize_messages(
            to_summarize,
            model,
            is_resummarize=has_existing_summary
        )

        if result is None:
            return messages, False

        summary_text, original_tokens, summary_tokens = result

        metadata = SummaryMetadata(
            created_at=time.strftime("%Y-%m-%d %H:%M:%S"),
            summarized_message_count=len(to_summarize),
            summarized_token_count=original_tokens,
            summary_token_count=summary_tokens,
            compression_ratio=summary_tokens / original_tokens if original_tokens > 0 else 0,
            model_used=model,
            summary_level=current_level + 1
        )

        summary_msg = SummaryMessage(
            role="assistant",
            content=f"[ì´ì „ ëŒ€í™” ìš”ì•½]\n\n{summary_text}",
            is_summary=True,
            metadata=metadata
        )

        self.summary_history.append(metadata)

        new_messages = [summary_msg.to_dict()] + to_keep

        return new_messages, True
